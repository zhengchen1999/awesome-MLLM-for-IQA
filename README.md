# awesome-MLLM-for-IQA
Collect MLLM-for-IQA related papers, data, repositories
## Contributing
Please feel free to pull request or email us to add papers
## papers
| Title           | New Model | New Dataset          | Published | Code      |
| --------------- |----------|----------------------|----------| --------- |
|[Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision](https://arxiv.org/abs/2309.14181)| None     | Q-Bench              | ICLR24   |[link](https://github.com/Q-Future/Q-Bench)|
|[Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models](https://arxiv.org/abs/2311.06783)| None     | Q-Pathway,<br/>Q-Instruct |CVPR24|[link](https://github.com/Q-Future/Q-Instruct)|
|[Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels](https://arxiv.org/abs/2312.17090)| ONEALIGN         | None                 | ICML24   |[link](https://github.com/Q-Future/Q-Align)|
|
