# awesome-MLLM-for-IQA
Collect MLLM-for-IQA related papers, data, repositories
## Contributing
Please feel free to pull request or email us to add papers
## papers
| Title                                                                                                                              | New Model    | New Dataset                | Published    | Code      |
|------------------------------------------------------------------------------------------------------------------------------------|--------------|----------------------------|--------------| --------- |
| [Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision](https://arxiv.org/abs/2309.14181)                 | None         | Q-Bench                    | ICLR24       |[link](https://github.com/Q-Future/Q-Bench)|
| [Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models](https://arxiv.org/abs/2311.06783)          | None         | Q-Pathway,<br/>Q-Instruct  | CVPR24       |[link](https://github.com/Q-Future/Q-Instruct)|
| [Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels](https://arxiv.org/abs/2312.17090)                     | ONEALIGN     | None                       | ICML24       |[link](https://github.com/Q-Future/Q-Align)|
| [Depicting Beyond Scores:Advancing Image Quality Assessment through Multi-modal Language Models](https://arxiv.org/abs/2312.08962) | DepictQA     | M-BAPPS                    | ECCV24       |[link](https://github.com/XPixelGroup/DepictQA)|
| [A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment](https://arxiv.org/abs/2403.10854v2)       | None         | None                       | ECCV24       |[link](https://github.com/TianheWu/MLLMs-for-IQA)|    
| [Q-Ground: Image Quality Grounding with Large Multi-modality Models](https://arxiv.org/abs/2407.17035)    | None         | QGround-100K               | ACM MM24     |    [link](https://github.imc.re/Q-Future/Q-Ground) |
| [Towards Open-ended Visual Quality Comparison](https://arxiv.org/abs/2402.16641)| Co-Instruct  | Co-Instruct-562K, MICBench |ECCV24|[link](https://github.com/Q-Future/Co-Instruct)|
| [VisualCritic: Making LMMs Perceive Visual Quality Like Humans](https://arxiv.org/abs/2403.12806)| VisualCritic |None|arxiv24|None|
| [2AFC Prompting of Large Multimodal Models for Image Quality Assessment](https://arxiv.org/abs/2402.01162)| None         |None|TCSVT24|[link](https://github.com/h4nwei/2AFC-LMMs)|
| [Descriptive Image Quality Assessment in the Wild](https://arxiv.org/pdf/2405.18842)|DepictQA-Wild|DQ-495k|arxiv24|[link](https://github.com/XPixelGroup/DepictQA)|
| [Adaptive Image Quality Assessment via Teaching Large Multimodal Model to Compare](https://arxiv.org/abs/2405.19298)|None|None|arxiv24|[link](https://github.com/Q-Future/Compare2Score)|
